# Modern LLM Architecture Zoo

A technical comparison of the most prominent models in the GenAI ecosystem today.

---

## ðŸš€ The State of the Art (Visualized)

```mermaid
mindmap
  root(("LLM Zoo"))
    "Dense Models"
      "Llama-3 (Meta)"
        "GQA Attention"
        "15T Token Training"
      "Gemma-2 (Google)"
        "Logit Soft-Capping"
        "Distillation"
    "MoE Models"
      "Mixtral (Mistral)"
        "8x7B Sparse"
        "SWA Attention"
      "DeepSeek-V3 (DeepSeek)"
        "MLA Attention"
        "Multi-Head Latent Experts"
    "Multi-Modal"
      "GPT-4o (OpenAI)"
        "Unified Stream"
        "Low Latency Audio"
      "Claude 3.5 (Anthropic)"
        "Visual Reasoning"
        "Artifacts"
```

---

## 1. Llama-3 (Meta)
*The Open Standard*

*   **Architecture**: Dense Transformer.
*   **Attention**: **GQA** (Grouped Query Attention) with 8 KV heads.
*   **Key Insight**: Llama-3 proved that **over-training** (15T tokens for 8B params) creates a model that outperforms previous models 5-10x its size.

---

## 2. DeepSeek-V3 / R1 (DeepSeek)
*The Efficiency Frontier*

*   **Architecture**: **MLA** (Multi-head Latent Attention) + **Multi-head Latent Experts**.
*   **Concept**: Instead of dense matrices, every component uses low-rank compression.
*   **Key Insight**: DeepSeek-V3 is the first model to reach GPT-4o level performance while being significantly cheaper to train and serve, thanks to its hardware-aware sparse architecture.

---

## 3. Claude 3.5 Sonnet (Anthropic)
*The Agent Specialist*

*   **Key Feature**: Exceptional **instruction-following** and **coding precision**.
*   **Alignment**: Uses Constitutional AI (heavy internal reinforcement learning) to ensure safety and neutrality.

---

## 4. Modern Architecture Cheat Sheet

| Feature | Llama-3 | DeepSeek-V3 | Mixtral |
|---------|---------|-------------|---------|
| **Core** | Dense | MoE | MoE |
| **Attention** | GQA | MLA | SWA |
| **Active Params** | 70B | ~37B | ~13B |
| **Context** | 128K | 128K | 32K |

---

## 5. Advanced Q&A

### Q1: "Why would a company use a Dense model (Llama-3) instead of a more efficient MoE (DeepSeek)?"
> **Answer**: Reliability and Hardware. Dense models are more "stable"â€”they don't suffer from routing errors or expert imbalance. More importantly, they occupy less total VRAM. A 70B dense model fits in 140GB, whereas a 670B MoE (like DeepSeek) might only use 30B params for math but still requires massive VRAM to store the expert library.

### Q2: "What is 'Multi-Token Prediction' (MTP) in DeepSeek-V3?"
> **Answer**: Standard LLMs predict 1 token at a time. MTP predicts the next $N$ tokens in parallel. This is used during training to force the model to look further ahead and understand global sequence structure, leading to better planning and reasoning capabilities.

### Q3: "What is 'Logit Soft-Capping' in Gemma-2?"
> **Answer**: It's a technique to keep attention scores from becoming too large (exploding). It caps the scores within a range during training using a `tanh` function. This results in more stable training and better generalization for smaller models.

### Q4: "How does 'Native Multimodality' differ from 'Bridged Multimodality'?"
> **Answer**: In **Bridged** (e.g., LLaVA), you have a separate Vision Encoder and an LLM. You "glue" them together with a projection layer. In **Native** (e.g., GPT-4o), the images, audio, and text are all converted to the same token space and processed by the **same transformer layers**. This allows for much higher nuance and cross-modal reasoning.

### Q5: "Which architecture is best for a 'Tool-Use' Agent?"
> **Answer**: Currently, models with **strong instruction-following alignment** and **low-latency GQA/MLA** are best. Claude 3.5 Sonnet and DeepSeek-V3 are the favorites because their attention mechanisms allow for massive "System Prompts" (filled with tool definitions) without slowing down the initial response (TTFT).
